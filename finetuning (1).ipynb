{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13458203,"sourceType":"datasetVersion","datasetId":8542796}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y transformers accelerate peft huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T09:08:42.281324Z","iopub.execute_input":"2025-10-27T09:08:42.281595Z","iopub.status.idle":"2025-10-27T09:08:43.843067Z","shell.execute_reply.started":"2025-10-27T09:08:42.281575Z","shell.execute_reply":"2025-10-27T09:08:43.842086Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.36.2\nUninstalling transformers-4.36.2:\n  Successfully uninstalled transformers-4.36.2\nFound existing installation: accelerate 0.23.0\nUninstalling accelerate-0.23.0:\n  Successfully uninstalled accelerate-0.23.0\nFound existing installation: peft 0.7.1\nUninstalling peft-0.7.1:\n  Successfully uninstalled peft-0.7.1\nFound existing installation: huggingface-hub 0.20.3\nUninstalling huggingface-hub-0.20.3:\n  Successfully uninstalled huggingface-hub-0.20.3\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip uninstall -y transformers huggingface_hub sentence-transformers accelerate datasets scikit-learn nltk pyarrow numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T09:08:43.844586Z","iopub.execute_input":"2025-10-27T09:08:43.844844Z","iopub.status.idle":"2025-10-27T09:08:47.802028Z","shell.execute_reply.started":"2025-10-27T09:08:43.844822Z","shell.execute_reply":"2025-10-27T09:08:47.801010Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping huggingface_hub as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: sentence-transformers 2.2.2\nUninstalling sentence-transformers-2.2.2:\n  Successfully uninstalled sentence-transformers-2.2.2\n\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: datasets 2.14.6\nUninstalling datasets-2.14.6:\n  Successfully uninstalled datasets-2.14.6\nFound existing installation: scikit-learn 1.3.2\nUninstalling scikit-learn-1.3.2:\n  Successfully uninstalled scikit-learn-1.3.2\nFound existing installation: nltk 3.8.1\nUninstalling nltk-3.8.1:\n  Successfully uninstalled nltk-3.8.1\nFound existing installation: pyarrow 14.0.2\nUninstalling pyarrow-14.0.2:\n  Successfully uninstalled pyarrow-14.0.2\nFound existing installation: numpy 1.26.4\nUninstalling numpy-1.26.4:\n  Successfully uninstalled numpy-1.26.4\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install numpy==1.26.4 --quiet\n!pip install scikit-learn==1.3.2 --quiet\n!pip install pyarrow==14.0.2 --quiet\n!pip install nltk==3.8.1 --quiet\n!pip install transformers==4.36.2 --quiet\n!pip install huggingface_hub==0.20.3 --quiet\n!pip install sentence-transformers==2.2.2 --quiet\n!pip install accelerate==0.23.0 --quiet\n!pip install datasets==2.14.6 --quiet\n!pip install peft==0.7.1\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T09:56:08.944786Z","iopub.execute_input":"2025-10-27T09:56:08.945380Z","iopub.status.idle":"2025-10-27T09:58:21.840072Z","shell.execute_reply.started":"2025-10-27T09:56:08.945349Z","shell.execute_reply":"2025-10-27T09:58:21.839086Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\ntextblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 14.0.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.20.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 14.0.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.20.3 which is incompatible.\npeft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.3 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.20.3 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\naccelerate 1.9.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npeft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.3 which is incompatible.\naccelerate 1.9.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.3 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npeft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.20.3 which is incompatible.\npeft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.20.3 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.20.3 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting peft==0.7.1\n  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (7.1.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (4.36.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (0.23.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (0.5.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.1) (0.20.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (2023.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1) (4.15.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft==0.7.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft==0.7.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft==0.7.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft==0.7.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft==0.7.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft==0.7.1) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.1) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.7.1) (2025.9.18)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.7.1) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft==0.7.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft==0.7.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft==0.7.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft==0.7.1) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.1) (2025.8.3)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft==0.7.1) (2024.2.0)\nDownloading peft-0.7.1-py3-none-any.whl (168 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\nSuccessfully installed peft-0.7.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy, sklearn, pyarrow, nltk, torch\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import load_dataset\nprint(\"âœ… All libraries imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:14:34.947227Z","iopub.execute_input":"2025-10-27T10:14:34.947899Z","iopub.status.idle":"2025-10-27T10:14:42.615304Z","shell.execute_reply.started":"2025-10-27T10:14:34.947859Z","shell.execute_reply":"2025-10-27T10:14:42.614678Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"},{"name":"stdout","text":"âœ… All libraries imported successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\nprint(\"Model loaded successfully âœ…\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:14:44.179522Z","iopub.execute_input":"2025-10-27T10:14:44.180031Z","iopub.status.idle":"2025-10-27T10:14:53.545188Z","shell.execute_reply.started":"2025-10-27T10:14:44.180007Z","shell.execute_reply":"2025-10-27T10:14:53.544369Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bbbafa8f0f64a3db4b6dba5f2169ad3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee59d74809784472ab2d5caeeb9545c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e6a23670cc46739d20019f8d368b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6adc2f5d6f344af6adbae811d4bf7068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901df85b110b4eb291c996aaad7d3377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f972ae3cc0445c9a59128130f341d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c76d9919004f2cb0af3c41f340a53c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c41c97b964ac4e4d90a6c07c1e97d8a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d05fde8606465ba9fcce2eed3f0902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a5852ce4d6426681ffd05942b9b28a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"870f80f9fe09498ca8a4e4f3f6193630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f553bd380f142cd8d042cc467cc9f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f330cfa9eda1479b9cb98aada94fbbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6db8293b1d654cc18c6b1625ca737254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0131f6d9ad7646a781515789cda5214c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f3572040b304fd8a4cfa02acdf95e36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model.xml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72630ea5402d454eafdc075ec0603b2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"317428ae39a741bf92b6dcbf9687e03f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model_qint8_quantized.xml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137332663c524433b56889ef567cbd57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d992762240984d3599b188c02897f810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"082d33485c1f4479a5d65943cb8b57a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a601375bfa1470b807d12c92b757e7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83a6b1a0fd294a40b0bd5b6439a0a386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1006a01735c84d5ba366a0c9eb472684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d710d168455c47d8ba7dd6eb7242c3af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db23d1a16f664e5e9a3ceb388322955a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully âœ…\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom tqdm import tqdm\n\n\ndf = pd.read_csv(\"/kaggle/input/train-csv/train.csv\")\nprint(\"âœ… Dataset loaded successfully!\")\nprint(df.head(2))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:14:55.879708Z","iopub.execute_input":"2025-10-27T10:14:55.880527Z","iopub.status.idle":"2025-10-27T10:14:57.188778Z","shell.execute_reply.started":"2025-10-27T10:14:55.880501Z","shell.execute_reply":"2025-10-27T10:14:57.188171Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset loaded successfully!\n                                         resume_text  \\\n0  SummaryHighly motivated Sales Associate with e...   \n1  Professional SummaryCurrently working with Cat...   \n\n                                job_description_text   label  \n0  Net2Source Inc. is an award-winning total work...  No Fit  \n1  At Salas OBrien we tell our clients that were ...  No Fit  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ----------------------------\n# 2. Clean text helper\n# ----------------------------\ndef clean_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n    return re.sub(r\"\\s+\", \" \", text).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:14:59.326688Z","iopub.execute_input":"2025-10-27T10:14:59.327317Z","iopub.status.idle":"2025-10-27T10:14:59.331299Z","shell.execute_reply.started":"2025-10-27T10:14:59.327288Z","shell.execute_reply":"2025-10-27T10:14:59.330599Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ----------------------------\n# 3. Build skill keywords dynamically from dataset\n# ----------------------------\nfrom collections import Counter\n\ndef build_skill_keywords(texts, top_n=200):\n    all_words = []\n    for text in texts:\n        text = clean_text(text)\n        all_words.extend(text.split())\n\n    # Remove common stopwords\n    common_words = set([\n        'the','and','to','a','of','in','for','with','on','as','is','are',\n        'at','an','by','from','that','this','be','we','our','their','your'\n    ])\n    \n    counter = Counter([w for w in all_words if w not in common_words])\n    keywords = [word for word, count in counter.most_common(top_n)]\n    return keywords\n\n# Build keywords from both resume_text + job_description_text\nall_texts = pd.concat([df['resume_text'], df['job_description_text']])\nskill_keywords = build_skill_keywords(all_texts)\n\nprint(\"âœ… Extracted skill keywords:\", len(skill_keywords))\nprint(\"ğŸ“˜ Example keywords:\", skill_keywords[:20])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:15:00.842458Z","iopub.execute_input":"2025-10-27T10:15:00.843079Z","iopub.status.idle":"2025-10-27T10:15:07.090283Z","shell.execute_reply.started":"2025-10-27T10:15:00.843057Z","shell.execute_reply":"2025-10-27T10:15:07.089397Z"}},"outputs":[{"name":"stdout","text":"âœ… Extracted skill keywords: 200\nğŸ“˜ Example keywords: ['data', 'experience', 'business', 'or', 'team', 'software', 'using', 'management', 'development', 'testing', 'work', 'sql', 'test', 'system', 'project', 's', 'all', 'systems', 'design', 'accounting']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ----------------------------\n# 4. Extract skills\n# ----------------------------\ndef extract_skills(text):\n    if pd.isna(text):\n        return []\n    text = clean_text(text)\n    tokens = text.split()\n    found = [w for w in tokens if w in skill_keywords]\n    return list(set(found))\n\ndf['cv_skills'] = df['resume_text'].apply(extract_skills)\ndf['jd_skills'] = df['job_description_text'].apply(extract_skills)\n\nprint(\"âœ… Skills extracted for CVs and JDs\")\ndf[['cv_skills', 'jd_skills']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:15:25.650567Z","iopub.execute_input":"2025-10-27T10:15:25.651253Z","iopub.status.idle":"2025-10-27T10:15:43.142391Z","shell.execute_reply.started":"2025-10-27T10:15:25.651225Z","shell.execute_reply":"2025-10-27T10:15:43.141582Z"}},"outputs":[{"name":"stdout","text":"âœ… Skills extracted for CVs and JDs\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                           cv_skills  \\\n0  [skills, inventory, business, university, comp...   \n1  [skills, business, support, create, teams, uni...   \n2  [inventory, business, into, company, provided,...   \n3  [skills, quality, which, team, business, creat...   \n4  [skills, oracle, business, support, create, us...   \n\n                                           jd_skills  \n0  [skills, which, or, business, time, sql, such,...  \n1  [skills, team, or, business, time, within, emp...  \n2  [skills, quality, team, or, such, employees, s...  \n3  [skills, team, or, business, have, benefits, w...  \n4  [skills, business, support, create, years, tec...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv_skills</th>\n      <th>jd_skills</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[skills, inventory, business, university, comp...</td>\n      <td>[skills, which, or, business, time, sql, such,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[skills, business, support, create, teams, uni...</td>\n      <td>[skills, team, or, business, time, within, emp...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[inventory, business, into, company, provided,...</td>\n      <td>[skills, quality, team, or, such, employees, s...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[skills, quality, which, team, business, creat...</td>\n      <td>[skills, team, or, business, have, benefits, w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[skills, oracle, business, support, create, us...</td>\n      <td>[skills, business, support, create, years, tec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ----------------------------\n# 5. Calculate skill overlap\n# ----------------------------\ndef skill_overlap(cv_skills, jd_skills):\n    cv_set = set(cv_skills)\n    jd_set = set(jd_skills)\n    if not jd_set:\n        return 0, [], []\n    \n    overlap = cv_set.intersection(jd_set)\n    missing = jd_set - cv_set\n    overlap_ratio = len(overlap) / len(jd_set)\n    return overlap_ratio, list(overlap), list(missing)\n\ntqdm.pandas()\ndf['skill_overlap_ratio'], df['common_skills'], df['missing_skills'] = zip(*df.progress_apply(\n    lambda row: skill_overlap(row['cv_skills'], row['jd_skills']), axis=1\n))\n\nprint(\"âœ… Skill overlap calculated!\")\npd.set_option('display.max_colwidth', 120)\ndf[['cv_skills', 'jd_skills', 'skill_overlap_ratio', 'common_skills', 'missing_skills']].head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:15:49.051287Z","iopub.execute_input":"2025-10-27T10:15:49.051861Z","iopub.status.idle":"2025-10-27T10:15:49.244983Z","shell.execute_reply.started":"2025-10-27T10:15:49.051812Z","shell.execute_reply":"2025-10-27T10:15:49.244183Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6241/6241 [00:00<00:00, 38956.13it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Skill overlap calculated!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                                                                                 cv_skills  \\\n0  [skills, inventory, business, university, company, responsibilities, tax, provided, use, technical, education, ms, o...   \n1  [skills, business, support, create, teams, university, into, company, engineer, technical, use, power, processes, he...   \n2  [inventory, business, into, company, provided, power, organization, knowledge, processes, education, services, opera...   \n3  [skills, quality, which, team, business, creating, equipment, time, such, or, procedures, computer, daily, including...   \n4  [skills, oracle, business, support, create, user, years, university, technologies, into, company, program, engineer,...   \n\n                                                                                                                 jd_skills  \\\n0  [skills, which, or, business, time, sql, such, employees, support, have, user, create, more, new, years, also, train...   \n1  [skills, team, or, business, time, within, employees, create, more, have, including, can, related, developed, years,...   \n2  [skills, quality, team, or, such, employees, support, computer, more, including, can, software, years, training, ben...   \n3  [skills, team, or, business, have, benefits, will, company, tax, communication, work, staff, you, services, role, en...   \n4  [skills, business, support, create, years, technologies, into, company, responsibilities, engineer, provided, techni...   \n\n   skill_overlap_ratio  \\\n0             0.478261   \n1             0.569444   \n2             0.354839   \n3             0.444444   \n4             0.597826   \n\n                                                                                                             common_skills  \\\n0  [skills, business, time, such, employees, more, new, also, training, company, status, worked, technical, use, net, s...   \n1  [skills, or, business, time, within, employees, create, have, including, related, developed, university, will, infor...   \n2  [quality, team, or, such, more, training, company, job, systems, work, power, knowledge, processes, up, experience, ...   \n3              [skills, team, or, business, strong, communication, work, environment, experience, other, degree, services]   \n4  [skills, quality, team, which, business, or, time, such, support, create, more, including, software, years, technolo...   \n\n                                                                                                            missing_skills  \n0  [which, or, sql, support, have, user, create, years, information, clients, agile, well, work, any, requirements, dat...  \n1  [team, more, can, years, also, clients, status, program, responsibilities, build, well, you, knowledge, education, d...  \n2  [skills, employees, support, computer, including, can, software, years, benefits, will, testing, information, engine...  \n3  [role, working, tax, health, accounting, have, staff, office, you, benefits, professional, will, microsoft, ability,...  \n4  [within, employees, benefits, will, clients, position, performance, responsibilities, job, build, provided, manage, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cv_skills</th>\n      <th>jd_skills</th>\n      <th>skill_overlap_ratio</th>\n      <th>common_skills</th>\n      <th>missing_skills</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[skills, inventory, business, university, company, responsibilities, tax, provided, use, technical, education, ms, o...</td>\n      <td>[skills, which, or, business, time, sql, such, employees, support, have, user, create, more, new, years, also, train...</td>\n      <td>0.478261</td>\n      <td>[skills, business, time, such, employees, more, new, also, training, company, status, worked, technical, use, net, s...</td>\n      <td>[which, or, sql, support, have, user, create, years, information, clients, agile, well, work, any, requirements, dat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[skills, business, support, create, teams, university, into, company, engineer, technical, use, power, processes, he...</td>\n      <td>[skills, team, or, business, time, within, employees, create, more, have, including, can, related, developed, years,...</td>\n      <td>0.569444</td>\n      <td>[skills, or, business, time, within, employees, create, have, including, related, developed, university, will, infor...</td>\n      <td>[team, more, can, years, also, clients, status, program, responsibilities, build, well, you, knowledge, education, d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[inventory, business, into, company, provided, power, organization, knowledge, processes, education, services, opera...</td>\n      <td>[skills, quality, team, or, such, employees, support, computer, more, including, can, software, years, training, ben...</td>\n      <td>0.354839</td>\n      <td>[quality, team, or, such, more, training, company, job, systems, work, power, knowledge, processes, up, experience, ...</td>\n      <td>[skills, employees, support, computer, including, can, software, years, benefits, will, testing, information, engine...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[skills, quality, which, team, business, creating, equipment, time, such, or, procedures, computer, daily, including...</td>\n      <td>[skills, team, or, business, have, benefits, will, company, tax, communication, work, staff, you, services, role, en...</td>\n      <td>0.444444</td>\n      <td>[skills, team, or, business, strong, communication, work, environment, experience, other, degree, services]</td>\n      <td>[role, working, tax, health, accounting, have, staff, office, you, benefits, professional, will, microsoft, ability,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[skills, oracle, business, support, create, user, years, university, technologies, into, company, program, engineer,...</td>\n      <td>[skills, business, support, create, years, technologies, into, company, responsibilities, engineer, provided, techni...</td>\n      <td>0.597826</td>\n      <td>[skills, quality, team, which, business, or, time, such, support, create, more, including, software, years, technolo...</td>\n      <td>[within, employees, benefits, will, clients, position, performance, responsibilities, job, build, provided, manage, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# ----------------------------\n# 6. Encode labels\n# ----------------------------\nlabel_map = {\n    \"No Fit\": 0,\n    \"Potential Fit\": 1,\n    \"Good Fit\": 2\n}\ndf[\"label\"] = df[\"label\"].map(label_map)\n\nprint(\"âœ… Label encoding complete!\")\nprint(df[\"label\"].value_counts())\n\ndf.to_csv(\"fine_tune_data_encoded.csv\", index=False)\nprint(\"ğŸ“ Saved as fine_tune_data_encoded.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:15:54.817514Z","iopub.execute_input":"2025-10-27T10:15:54.818099Z","iopub.status.idle":"2025-10-27T10:15:56.776081Z","shell.execute_reply.started":"2025-10-27T10:15:54.818076Z","shell.execute_reply":"2025-10-27T10:15:56.775316Z"}},"outputs":[{"name":"stdout","text":"âœ… Label encoding complete!\nlabel\n0    3143\n1    1556\n2    1542\nName: count, dtype: int64\nğŸ“ Saved as fine_tune_data_encoded.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\n\n# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\nmodel = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n\n\nsentences = [\n    \"Software engineer with experience in Python and data analysis.\",\n    \"We are hiring a Python developer with strong coding skills.\"\n]\n\nembeddings = model.encode(sentences)\nprint(\"âœ… Model loaded successfully!\")\nprint(\"Embeddings shape:\", embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:15:58.610365Z","iopub.execute_input":"2025-10-27T10:15:58.611052Z","iopub.status.idle":"2025-10-27T10:15:59.262430Z","shell.execute_reply.started":"2025-10-27T10:15:58.611028Z","shell.execute_reply":"2025-10-27T10:15:59.261567Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf9838948ba4265bddbb1962f461021"}},"metadata":{}},{"name":"stdout","text":"âœ… Model loaded successfully!\nEmbeddings shape: (2, 384)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# from datasets import load_dataset\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\n# # Load dataset\n# dataset = load_dataset('csv', data_files={'train': '/kaggle/working/fine_tune_data_encoded.csv'})['train']\n\n# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\n# # Tokenize\n# def tokenize_function(examples):\n#     return tokenizer(examples['resume_text'], examples['job_description_text'],\n#                      truncation=True, padding='max_length', max_length=512)\n\n# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n# tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# # Split data\n# train_size = int(0.9 * len(tokenized_dataset))\n# train_dataset = tokenized_dataset.select(range(train_size))\n# eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n\n# # Training setup\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     evaluation_strategy=\"epoch\",\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=2,\n#     per_device_eval_batch_size=2,\n#     num_train_epochs=2,\n#     weight_decay=0.01,\n#     save_strategy=\"epoch\",\n#     logging_dir='./logs',\n#     logging_steps=50\n# )\n\n# # Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset\n# )\n\n# trainer.train()\n\n# # Save\n# model.save_pretrained(\"./fine_tuned_job_match_model\")\n# tokenizer.save_pretrained(\"./fine_tuned_job_match_model\")\n\n# print(\"ğŸ¯ Fine-tuning completed and model saved successfully!\")\n#-----------------------------------------------------------------------------------------------------------------------------\n# from datasets import load_dataset\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n\n# # Load dataset\n# dataset = load_dataset('csv', data_files={'train': '/kaggle/working/fine_tune_data_encoded.csv'})['train']\n\n# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\n# # Tokenize function\n# def tokenize_function(examples):\n#     return tokenizer(\n#         examples['resume_text'],\n#         examples['job_description_text'],\n#         truncation=True,\n#         padding='max_length',\n#         max_length=512\n#     )\n\n# # Tokenize dataset\n# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n# tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# # Split data into train/eval\n# train_size = int(0.9 * len(tokenized_dataset))\n# train_dataset = tokenized_dataset.select(range(train_size))\n# eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n\n# # Training setup\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     evaluation_strategy=\"epoch\",\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=1,  # smaller for less GPU memory\n#     per_device_eval_batch_size=1,\n#     num_train_epochs=2,\n#     weight_decay=0.01,\n#     save_strategy=\"epoch\",\n#     logging_dir='./logs',\n#     logging_steps=50\n# )\n\n# # Initialize trainer (without training yet)\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=eval_dataset\n# )\n\n# print(\"âœ… Trainer is ready! You can now run the training cell.\")\n#____________________________________________________________________________________________\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport os\n\n# Disable WandB\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n# Load dataset\ndataset = load_dataset('csv', data_files={'train': '/kaggle/working/fine_tune_data_encoded.csv'})['train']\n\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['resume_text'], examples['job_description_text'],\n                     truncation=True, padding='max_length', max_length=512)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\ntrain_size = int(0.9 * len(tokenized_dataset))\ntrain_dataset = tokenized_dataset.select(range(train_size))\neval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    logging_steps=50\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:30:53.328667Z","iopub.execute_input":"2025-10-27T10:30:53.329031Z","iopub.status.idle":"2025-10-27T10:30:53.855946Z","shell.execute_reply.started":"2025-10-27T10:30:53.329009Z","shell.execute_reply":"2025-10-27T10:30:53.855339Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers.integrations import WandbCallback\ntrainer.pop_callback(WandbCallback)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:31:04.409217Z","iopub.execute_input":"2025-10-27T10:31:04.409682Z","iopub.status.idle":"2025-10-27T10:31:04.413223Z","shell.execute_reply.started":"2025-10-27T10:31:04.409660Z","shell.execute_reply":"2025-10-27T10:31:04.412494Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"dryrun\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:31:06.379395Z","iopub.execute_input":"2025-10-27T10:31:06.379900Z","iopub.status.idle":"2025-10-27T10:31:06.383642Z","shell.execute_reply.started":"2025-10-27T10:31:06.379876Z","shell.execute_reply":"2025-10-27T10:31:06.382987Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trainer.train()\n# Save model and tokenizer\n# model.save_pretrained(\"./fine_tuned_job_match_model\")\n# tokenizer.save_pretrained(\"./fine_tuned_job_match_model\")\n\nprint(\"ğŸ¯ Fine-tuning completed and model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:31:07.979899Z","iopub.execute_input":"2025-10-27T10:31:07.980500Z","iopub.status.idle":"2025-10-27T10:36:33.553797Z","shell.execute_reply.started":"2025-10-27T10:31:07.980475Z","shell.execute_reply":"2025-10-27T10:36:33.553049Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2808' max='2808' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2808/2808 05:24, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.938600</td>\n      <td>1.425070</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.848100</td>\n      <td>1.755977</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.733000</td>\n      <td>1.149525</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.672600</td>\n      <td>1.082672</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nCheckpoint destination directory ./results/checkpoint-1404 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nCheckpoint destination directory ./results/checkpoint-2808 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¯ Fine-tuning completed and model saved successfully!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:27:21.462777Z","iopub.execute_input":"2025-10-27T10:27:21.463394Z","iopub.status.idle":"2025-10-27T10:27:25.111412Z","shell.execute_reply.started":"2025-10-27T10:27:21.463370Z","shell.execute_reply":"2025-10-27T10:27:25.110649Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.6)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.20.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.12.15)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.19.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:27:54.432169Z","iopub.execute_input":"2025-10-27T10:27:54.432797Z","iopub.status.idle":"2025-10-27T10:27:57.653635Z","shell.execute_reply.started":"2025-10-27T10:27:54.432774Z","shell.execute_reply":"2025-10-27T10:27:57.652884Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.0a1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.3)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.5)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.37.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# from datasets import load_metric\n\n# # ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù€ accuracy\n# metric = load_metric(\"accuracy\")\n\n# # Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ (Ø¨ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ù€ Trainer)\n# def compute_metrics(eval_pred):\n#     logits, labels = eval_pred\n#     predictions = logits.argmax(axis=-1)\n#     return metric.compute(predictions=predictions, references=labels)\n\n# # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\n# eval_result = trainer.evaluate()\n# print(\"âœ… Evaluation completed!\")\n# print(\"ğŸ“Š Accuracy:\", eval_result[\"eval_accuracy\"])\n# print(\"ğŸ“‰ Loss:\", eval_result[\"eval_loss\"])\n\nimport evaluate  # âœ… Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† from datasets import load_metric\n\n# ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù€ accuracy\nmetric = evaluate.load(\"accuracy\")\n\n# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ (Ø¨ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ù€ Trainer)\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Ù„Ø§Ø²Ù… Ù†Ø¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Trainer ÙˆÙ†Ù…Ø±Ø± Ù„Ù‡ Ø¯Ø§Ù„Ø© compute_metrics\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics  # âœ… Ø¯ÙŠ Ø£Ù‡Ù… Ø®Ø·ÙˆØ©\n)\n\n# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\neval_result = trainer.evaluate()\nprint(\"âœ… Evaluation completed!\")\nprint(eval_result)  # Ø´ÙˆÙÙŠ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù„ÙŠ Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„ØªØ£ÙƒØ¯\nprint(\"ğŸ“Š Accuracy:\", eval_result[\"eval_accuracy\"])\nprint(\"ğŸ“‰ Loss:\", eval_result[\"eval_loss\"])\n\nimport evaluate  # âœ… Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† from datasets import load_metric\n\n# ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù€ accuracy\nmetric = evaluate.load(\"accuracy\")\n\n# Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ (Ø¨ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ù€ Trainer)\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Ù„Ø§Ø²Ù… Ù†Ø¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Trainer ÙˆÙ†Ù…Ø±Ø± Ù„Ù‡ Ø¯Ø§Ù„Ø© compute_metrics\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics  # âœ… Ø¯ÙŠ Ø£Ù‡Ù… Ø®Ø·ÙˆØ©\n)\n\n# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\neval_result = trainer.evaluate()\nprint(\"âœ… Evaluation completed!\")\nprint(eval_result)  # Ø´ÙˆÙÙŠ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù„ÙŠ Ø±Ø§Ø¬Ø¹Ø© Ù„Ù„ØªØ£ÙƒØ¯\nprint(\"ğŸ“Š Accuracy:\", eval_result[\"eval_accuracy\"])\nprint(\"ğŸ“‰ Loss:\", eval_result[\"eval_loss\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:28:02.430823Z","iopub.execute_input":"2025-10-27T10:28:02.431155Z","iopub.status.idle":"2025-10-27T10:28:02.894572Z","shell.execute_reply.started":"2025-10-27T10:28:02.431131Z","shell.execute_reply":"2025-10-27T10:28:02.893566Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/570926044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Ù„Ø§Ø²Ù… Ù†Ø¹ÙŠØ¯ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù€ Trainer ÙˆÙ†Ù…Ø±Ø± Ù„Ù‡ Ø¯Ø§Ù„Ø© compute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_CALLBACKS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         self.callback_handler = CallbackHandler(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36madd_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mcb_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mhas_wandb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_wandb_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WandbCallback requires wandb to be installed. Run `pip install wandb`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: WandbCallback requires wandb to be installed. Run `pip install wandb`."],"ename":"RuntimeError","evalue":"WandbCallback requires wandb to be installed. Run `pip install wandb`.","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"import os\nimport evaluate\nfrom transformers import Trainer, TrainingArguments\n\n# ğŸ”¹ ØªØ¹Ø·ÙŠÙ„ Ø£ÙŠ ØªÙƒØ§Ù…Ù„ Ù…Ø¹ WandB\nos.environ[\"WANDB_MODE\"] = \"disabled\"\n\n# ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©\naccuracy_metric = evaluate.load(\"accuracy\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\nf1_metric = evaluate.load(\"f1\")\n\n# ğŸ”¹ Ø¯Ø§Ù„Ø© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    \n    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n    precision = precision_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n    recall = recall_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n    \n    return {\n        \"accuracy\": accuracy[\"accuracy\"],\n        \"precision\": precision[\"precision\"],\n        \"recall\": recall[\"recall\"],\n        \"f1\": f1[\"f1\"]\n    }\n\n# ğŸ”¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    logging_steps=50,\n    report_to=\"none\"  # âœ… ÙŠÙ…Ù†Ø¹ WandB Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§\n)\n\n# ğŸ”¹ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù€ Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics\n)\n\n# ğŸ”¹ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„\neval_result = trainer.evaluate()\nprint(\"âœ… Evaluation completed successfully!\")\nprint(\"ğŸ“Š Results:\")\nfor k, v in eval_result.items():\n    print(f\"  {k}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T10:55:00.356807Z","iopub.execute_input":"2025-10-27T10:55:00.357488Z","iopub.status.idle":"2025-10-27T10:55:05.325914Z","shell.execute_reply.started":"2025-10-27T10:55:00.357464Z","shell.execute_reply":"2025-10-27T10:55:05.325171Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"âœ… Evaluation completed successfully!\nğŸ“Š Results:\n  eval_loss: 1.082672119140625\n  eval_accuracy: 0.6144\n  eval_precision: 1.0\n  eval_recall: 0.6144\n  eval_f1: 0.7611496531219029\n  eval_runtime: 3.19\n  eval_samples_per_second: 195.925\n  eval_steps_per_second: 24.765\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}